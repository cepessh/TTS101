{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 1 Homework: NLTK basics and stopwords\n",
    "\n",
    "This homework lets you practise the ideas from Lesson 1 using a longer,\n",
    "real text about Robin Hood. You will:\n",
    "\n",
    "- Load text from a file\n",
    "- Tokenize into sentences and words\n",
    "- Remove punctuation\n",
    "- Handle stopwords\n",
    "- Try stemming and lemmatization\n",
    "- Explore pronunciations with `cmudict` (preview for TTS)\n",
    "\n",
    "Cells with `TODO` comments are for you to fill in. Read the hint in\n",
    "each cell before you start typing code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dee87ee",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    "\n",
    "Run this cell once to make sure all NLTK resources needed for this\n",
    "homework are available in your environment. It will download:\n",
    "\n",
    "- `punkt` \u2013 sentence tokeniser data\n",
    "- `averaged_perceptron_tagger` / `averaged_perceptron_tagger_eng` \u2013 POS tagger models\n",
    "- `cmudict` \u2013 CMU pronouncing dictionary (for phones)\n",
    "- `wordnet` and `omw-1.4` \u2013 data for lemmatisation\n",
    "- `stopwords` \u2013 lists of common function words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61473ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the NLTK data used in this homework.\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"averaged_perceptron_tagger_eng\")\n",
    "nltk.download(\"cmudict\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "nltk.download(\"stopwords\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1d6897",
   "metadata": {},
   "source": [
    "## 1. Load the Robin Hood text\n",
    "\n",
    "We will work with a real literary text from *The Merry Adventures of\n",
    "Robin Hood* (public domain, Project Gutenberg). The text is stored in\n",
    "the file `example_text_robin_hood.txt` in the same folder as this\n",
    "notebook.\n",
    "\n",
    "Here we use `pathlib.Path(...).read_text()` instead of a context\n",
    "manager with `open(...)` because it is a compact, modern way to read\n",
    "an entire file as a string, and it automatically closes the file for\n",
    "us. The `open`/`with` pattern is equivalent but more verbose; you can\n",
    "always rewrite the code using `with open(...) as f:` if you prefer.\n",
    "\n",
    "### Task\n",
    "\n",
    "- Use `pathlib.Path` and `.read_text()` to load the contents of the\n",
    "  file into a variable named `text`.\n",
    "- Print the first 500 characters to get a feeling for the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# TODO: read the contents of \"example_text_robin_hood.txt\" into a\n",
    "# variable called `text` using Path(...).read_text(encoding=\"utf-8\").\n",
    "# TODO: print the first 500 characters of `text`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sentence tokenization with PunktSentenceTokenizer\n",
    "\n",
    "In the lesson notebook you used `PunktSentenceTokenizer` to split a\n",
    "short sample into sentences. Here you will do the same with the longer\n",
    "Robin Hood text.\n",
    "\n",
    "### Task\n",
    "\n",
    "- Create a `PunktSentenceTokenizer` object.\n",
    "- Use its `.tokenize()` method to split `text` into a list named\n",
    "  `sentences`.\n",
    "- Inspect the number of sentences and look at the first few."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "# TODO: create a PunktSentenceTokenizer and use its .tokenize(text)\n",
    "# method to build a list called `sentences`.\n",
    "# TODO: inspect how many sentences you get and look at the first few.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Word tokenization with TreebankWordTokenizer\n",
    "\n",
    "Next, you will break the text into word-level tokens **and** keep track\n",
    "of which words belong to which sentence. This is useful later for\n",
    "text-to-speech, because the system usually speaks one sentence at a\n",
    "time and needs to know where sentences begin and end to control\n",
    "pauses and prosody.\n",
    "\n",
    "### Task\n",
    "\n",
    "- Use `TreebankWordTokenizer` (as in the lesson) **together with the\n",
    "  `sentences` list from the previous step**.\n",
    "- Build a list of lists called `words_per_sentence`, where each inner\n",
    "  list contains the tokens for one sentence.\n",
    "- Create a lowercased version of this grouped list called\n",
    "  `lower_words_per_sentence`.\n",
    "- Optionally also create a single flat list `words` containing all\n",
    "  tokens, and a flat lowercased list `lower_words` that you can reuse\n",
    "  later.\n",
    "\n",
    "Hint: you will probably need a **nested loop** (or a list\n",
    "comprehension with another loop inside it) that goes over sentences\n",
    "first and then over the tokens inside each sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "# TODO: use TreebankWordTokenizer together with the `sentences` list\n",
    "#       to build `words_per_sentence` (a list of lists of tokens).\n",
    "# TODO: build a lowercased version called `lower_words_per_sentence`.\n",
    "# TODO: optionally create flat lists `words` and `lower_words` that\n",
    "#       contain all tokens from all sentences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Removing punctuation with RegexpTokenizer (review)\n",
    "\n",
    "You previously saw how `TreebankWordTokenizer` splits text into\n",
    "words *and* keeps punctuation as separate tokens (for example,\n",
    "`\"Robin,\"` becomes `[\"Robin\", \",\"]`).\n",
    "\n",
    "In contrast, `RegexpTokenizer(r\"\\w+\")` keeps only word-like\n",
    "tokens and drops punctuation entirely. In real projects you would\n",
    "normally pick **one** tokenizer style or the other, depending on\n",
    "your goal, rather than using both at the same time. Here we use\n",
    "both only to compare their behaviour.\n",
    "\n",
    "For text-to-speech it is helpful to have **clean word tokens per\n",
    "sentence**, so in this section we remove punctuation at the\n",
    "sentence level using the `sentences` list from section 2.\n",
    "\n",
    "### Task\n",
    "\n",
    "- Create a `RegexpTokenizer` with the pattern `r\"\\w+\"`.\n",
    "- For each sentence in `sentences`, use the tokenizer to build a list\n",
    "  of tokens without punctuation. Collect these into a list of lists\n",
    "  called `words_no_punct_per_sentence`.\n",
    "- Optionally also make a flat list `words_no_punct` with all\n",
    "  punctuation-free tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# TODO: create a RegexpTokenizer with the pattern r\"\\w+\".\n",
    "# TODO: using the `sentences` list, build `words_no_punct_per_sentence`\n",
    "#       (tokens per sentence, no punctuation). Hint: use a nested loop\n",
    "#       or a list comprehension over sentences.\n",
    "# TODO: if you create a flat list `words_no_punct`, you can also\n",
    "#       compare its length to `words`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Stopwords (optional)\n",
    "\n",
    "Stopwords are very common words (like \"the\", \"and\", \"to\") that often\n",
    "do not carry much meaning by themselves. NLTK provides a list of\n",
    "English stopwords.\n",
    "This section is **optional** and mainly useful for text analysis; a\n",
    "basic speech-generation system would normally keep these words so that\n",
    "sentences sound natural.\n",
    "\n",
    "### Task\n",
    "\n",
    "- Import `stopwords` from `nltk.corpus`.\n",
    "- Build a Python `set` of English stopwords.\n",
    "- Starting from `words_no_punct_per_sentence`, create a list\n",
    "  `content_words_per_sentence` that excludes any token whose\n",
    "  lowercase form is in the stopword set. Optionally also build a\n",
    "  flat list `content_words`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# TODO: build a set of English stopwords using stopwords.words(\"english\").\n",
    "# TODO: starting from `words_no_punct`, create a list `content_words`\n",
    "#       that excludes any token whose lowercase form is in the\n",
    "#       stopword set.\n",
    "# TODO: print how many tokens remain after removing stopwords.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Stemming and lemmatization on content words (optional)\n",
    "\n",
    "Finally, you will apply a stemmer and a lemmatizer to some\n",
    "**content words** from the Robin Hood text that you choose yourself.\n",
    "This section is **optional**. Stemming and lemmatization are very\n",
    "useful for analysing language, but a basic text-to-speech system\n",
    "usually works with the original word forms.\n",
    "\n",
    "### Task\n",
    "\n",
    "- Use `PorterStemmer` to compute stems for some frequent\n",
    "  content words (for example from a flattened version of\n",
    "  `content_words_per_sentence`).\n",
    "- Use `WordNetLemmatizer` to compute lemmas for the same words.\n",
    "- Compare stems and lemmas for a few examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# TODO: choose a small list of content words, for example by picking\n",
    "#       some interesting nouns or verbs from `content_words`.\n",
    "# TODO: create a list of stems using PorterStemmer and a list of\n",
    "#       lemmas using WordNetLemmatizer.\n",
    "# TODO: display (word, stem, lemma) triples for your sample.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. CMU Pronouncing Dictionary (`cmudict`)\n",
    "\n",
    "For text-to-speech we need to know **how words are pronounced**. NLTK\n",
    "includes the CMU Pronouncing Dictionary (`cmudict`), which maps many\n",
    "English words to lists of phones.\n",
    "\n",
    "### Task\n",
    "\n",
    "- Import `cmudict` from `nltk.corpus`.\n",
    "- Build the dictionary with `cmudict.dict()`.\n",
    "- Pick a small list of words from the Robin Hood text (for example\n",
    "  `[\"forest\", \"sheriff\", \"arrow\", \"king\"]`).\n",
    "- Look up each word in the dictionary and print the word together with\n",
    "  its pronunciation(s).\n",
    "- For each word, also print how many phones are in its first\n",
    "  pronunciation (use `len(...)`).\n",
    "- (Optional, a bit trickier) Approximate the number of **stressed\n",
    "  syllables** by counting phones that end in a digit (like `AA1`,\n",
    "  `EH2`). Which words sound longer or more complex according to this\n",
    "  simple measure?\n",
    "- Choose one short sentence from `words_no_punct_per_sentence` and, for\n",
    "  each word in that sentence, look up its phones (skipping words that\n",
    "  are missing). Build a list `phones_for_sentence` that contains all\n",
    "  phones in order.\n",
    "- From `phones_for_sentence`, build a list of simple **diphone labels**\n",
    "  (pairs of neighbouring phones). This is close to what a diphone-based\n",
    "  TTS system would need before matching phones to pieces of audio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e04aa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import cmudict\n",
    "\n",
    "# TODO: create the CMU dictionary object (for example, cmu = cmudict.dict()).\n",
    "# TODO: choose a list of words from the text and print each word together\n",
    "#       with its entry in the dictionary (use cmu.get(word.lower())).\n",
    "# TODO: for the first pronunciation of each word, also print how many\n",
    "#       phones it contains.\n",
    "# TODO (optional): count how many phones end with a digit to estimate\n",
    "#       the number of stressed syllables.\n",
    "# TODO: pick one short sentence from `words_no_punct_per_sentence` and\n",
    "#       build a list `phones_for_sentence` by concatenating the phone\n",
    "#       sequences for each word (skipping missing words). Then build a\n",
    "#       list of diphone labels from `phones_for_sentence`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}