{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 1: Introduction to NLTK\n",
    "\n",
    "In this lesson we will start learning basic text processing with **NLTK** (Natural Language Toolkit).\n",
    "\n",
    "The goals for today are:\n",
    "- Install and import NLTK\n",
    "- Load some example text\n",
    "- Tokenize text into sentences and words\n",
    "- Explore simple frequency counts\n",
    "- Practice with a few short exercises\n",
    "\n",
    "> This notebook is designed for step-by-step practice. You can run cells one by one and discuss the outputs as you go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "In this project we will use a **virtual environment (venv)** so the packages for MariaTTS do not affect your system Python.\n",
    "\n",
    "### 1.1 Create a virtual environment (in the terminal)\n",
    "\n",
    "In a terminal inside this project folder, run:\n",
    "\n",
    "```bash\n",
    "python3 -m venv .venv\n",
    "source .venv/bin/activate  # macOS / Linux\n",
    "# On Windows (PowerShell):\n",
    "# .venv\\\\Scripts\\\\Activate.ps1\n",
    "```\n",
    "\n",
    "You should now see `(.venv)` at the start of your terminal prompt.\n",
    "\n",
    "Then install the packages we will use in this course:\n",
    "\n",
    "```bash\n",
    "pip install jupyter nltk numpy pyaudio\n",
    "```\n",
    "\n",
    "When you start Jupyter, make sure the kernel (Python interpreter) is the one from this `.venv`.\n",
    "\n",
    "The next cell installs NLTK **inside** whichever environment your notebook is using. If NLTK is already installed in that environment, you can skip it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c12b0b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in ./venv/lib/python3.13/site-packages (3.9.2)\n",
      "Requirement already satisfied: click in ./venv/lib/python3.13/site-packages (from nltk) (8.3.1)\n",
      "Requirement already satisfied: joblib in ./venv/lib/python3.13/site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./venv/lib/python3.13/site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in ./venv/lib/python3.13/site-packages (from nltk) (4.67.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install NLTK if needed (may already be installed)\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba152ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/liv/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/liv/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package cmudict to /home/liv/nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/liv/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to /home/liv/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the NLTK data we will need\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")        # older tagger name\n",
    "nltk.download(\"averaged_perceptron_tagger_eng\")    # newer tagger name used by pos_tag\n",
    "nltk.download(\"cmudict\")  # pronunciation dictionary used later for TTS\n",
    "nltk.download(\"wordnet\")   # needed for WordNetLemmatizer\n",
    "nltk.download(\"omw-1.4\")   # extra WordNet data (optional but useful)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da56aacd",
   "metadata": {},
   "source": [
    "## 2. Loading some text\n",
    "\n",
    "To experiment, we will start with a short sample text. Later, you can replace this with text you choose (e.g. a paragraph from a story or article)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cb7b3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text processing with Python is fun. We can use NLTK to explore language. This will be useful for future projects!\n"
     ]
    }
   ],
   "source": [
    "sample_text = (\n",
    "    \"Text processing with Python is fun. \"\n",
    "    \"We can use NLTK to explore language. \"\n",
    "    \"This will be useful for future projects!\"\n",
    ")\n",
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea34589",
   "metadata": {},
   "source": [
    "### Exercise 1 (discussion)\n",
    "\n",
    "- Read the text above out loud.\n",
    "- Think about what *tokens* might mean in the context of text.\n",
    "- How might a computer split this text into smaller pieces?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030bd6e6",
   "metadata": {},
   "source": [
    "## 3. Sentence tokenization\n",
    "\n",
    "NLTK provides tools for splitting text into sentences.\n",
    "\n",
    "Here we use **PunktSentenceTokenizer**, which is a pre-trained model\n",
    "that has learned how sentences usually start and end based on large\n",
    "collections of English text. It handles things like periods in\n",
    "abbreviations (e.g. \"Dr.\") better than a simple `split('.')` would."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d03e1a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Text processing with Python is fun.',\n",
       " 'We can use NLTK to explore language.',\n",
       " 'This will be useful for future projects!']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "sentence_tokenizer = PunktSentenceTokenizer()\n",
    "sentences = sentence_tokenizer.tokenize(sample_text)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8530c24e",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "1. How many sentences did NLTK find?\n",
    "2. Does this match what *you* consider separate sentences?\n",
    "3. Replace `sample_text` with a short text of your own choice and run the cells again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b3c5fb",
   "metadata": {},
   "source": [
    "## 4. Word tokenization\n",
    "\n",
    "Next, we split the text into **word tokens**. This will be important later when we want to count words or analyze patterns in language.\n",
    "\n",
    "Here we use **TreebankWordTokenizer**, which follows rules that were\n",
    "designed for a well-known annotated corpus (the Penn Treebank).\n",
    "It knows, for example, how to separate contractions (`\"don't\" → \"do\" + \"n't\"`)\n",
    "and how to handle punctuation around words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4beafead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Text',\n",
       " 'processing',\n",
       " 'with',\n",
       " 'Python',\n",
       " 'is',\n",
       " 'fun.',\n",
       " 'We',\n",
       " 'can',\n",
       " 'use',\n",
       " 'NLTK',\n",
       " 'to',\n",
       " 'explore',\n",
       " 'language.',\n",
       " 'This',\n",
       " 'will',\n",
       " 'be',\n",
       " 'useful',\n",
       " 'for',\n",
       " 'future',\n",
       " 'projects',\n",
       " '!']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "word_tokenizer = TreebankWordTokenizer()\n",
    "words = word_tokenizer.tokenize(sample_text)\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a41bb2",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "- Look at the list of tokens above.\n",
    "- Which tokens are words, and which are punctuation?\n",
    "- Do you see anything surprising, or something you would handle differently if you were tokenizing by hand?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203dc7ea",
   "metadata": {},
   "source": [
    "## 5. Normalizing text (lowercasing)\n",
    "\n",
    "A common step in text processing is to convert everything to lowercase so that `Text` and `text` are treated the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d325882a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text',\n",
       " 'processing',\n",
       " 'with',\n",
       " 'python',\n",
       " 'is',\n",
       " 'fun.',\n",
       " 'we',\n",
       " 'can',\n",
       " 'use',\n",
       " 'nltk',\n",
       " 'to',\n",
       " 'explore',\n",
       " 'language.',\n",
       " 'this',\n",
       " 'will',\n",
       " 'be',\n",
       " 'useful',\n",
       " 'for',\n",
       " 'future',\n",
       " 'projects',\n",
       " '!']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lower_words = [w.lower() for w in words]\n",
    "lower_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7d1b87",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "- Why might lowercasing be helpful when counting word frequencies?\n",
    "- Can you think of any situation where lowercasing might *lose* useful information?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91c84bb",
   "metadata": {},
   "source": [
    "## 6. Counting word frequencies\n",
    "\n",
    "NLTK has a convenient `FreqDist` class for counting how often each token appears. This is a simple but powerful tool that we will reuse later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad336233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text', 1),\n",
       " ('processing', 1),\n",
       " ('with', 1),\n",
       " ('python', 1),\n",
       " ('is', 1),\n",
       " ('fun.', 1),\n",
       " ('we', 1),\n",
       " ('can', 1),\n",
       " ('use', 1),\n",
       " ('nltk', 1)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "freq = FreqDist(lower_words)\n",
    "freq.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dbaba1",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "1. Which words are most common in this tiny sample?\n",
    "2. Change `sample_text` to a different paragraph and run the cells again.\n",
    "3. Compare the most common words between two different texts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d18fad7",
   "metadata": {},
   "source": [
    "## 7. Dealing with punctuation\n",
    "\n",
    "Sometimes we want to remove punctuation tokens so that we only count *words*.\n",
    "\n",
    "NLTK also provides tokenizers that can help with this. Here we use\n",
    "**RegexpTokenizer** with a pattern that keeps only word characters\n",
    "(`\\w+` = letters, digits and underscore) and drops punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe71b337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "no_punct_tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "# TODO: use `no_punct_tokenizer` to create a list called\n",
    "# `words_no_punct` from `sample_text` that contains only\n",
    "# tokens without punctuation.\n",
    "\n",
    "words_no_punct = []  # TODO: replace this with your solution\n",
    "words_no_punct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6\n",
    "\n",
    "- Complete the TODO above to fill in `words_no_punct`.\n",
    "- Compare `words` and `words_no_punct`. Which tokens disappeared?\n",
    "- Rerun this with your own `sample_text` and see what happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Stemming\n",
    "\n",
    "A **stemmer** reduces related word forms to a shorter root form.\n",
    "For example, `\"connect\"`, `\"connected\"`, and `\"connection\"` may all\n",
    "be reduced to something like `\"connect\"`.\n",
    "\n",
    "NLTK includes several stemmers. Here we will use the **PorterStemmer**,\n",
    "a classic rule-based algorithm that chops off common English endings\n",
    "(such as `-ing`, `-ed`, `-s`). It is simple and fast, and often used as\n",
    "a first step for grouping related word forms in text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# TODO: apply the stemmer to each token in `lower_words`\n",
    "# and store the result in a new list called `stems`.\n",
    "\n",
    "stems = []  # TODO: replace this with your solution\n",
    "stems[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7\n",
    "\n",
    "- Complete the TODO above to create the list `stems`.\n",
    "- Pick a few words and compare the original token to its stem.\n",
    "- Which stems look reasonable? Which ones look a bit strange?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Lemmatization\n",
    "\n",
    "A **lemmatizer** tries to reduce words to a base form that is a real\n",
    "dictionary word (a *lemma*). This often works better if we know the\n",
    "part-of-speech of each word.\n",
    "\n",
    "Here we use WordNetLemmatizer for a small subset of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# TODO: lemmatize a small list of example words.\n",
    "# Start with verbs and nouns like \"running\", \"better\", \"mice\", \"was\".\n",
    "\n",
    "example_words = [\"running\", \"better\", \"mice\", \"was\"]  # you can edit this list\n",
    "\n",
    "lemmas = []  # TODO: fill this list using lemmatizer\n",
    "list(zip(example_words, lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8\n",
    "\n",
    "- Complete the TODO above to fill in `lemmas`.\n",
    "- Compare stems vs. lemmas for the same words (for example, by\n",
    "  feeding the same list into your stemmer code).\n",
    "- Which representation do you think would be more useful for\n",
    "  analyzing text in your projects?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Pronunciations for speech (CMU dictionary)\n",
    "\n",
    "For text-to-speech we eventually need to know **how to pronounce each word**.\n",
    "NLTK includes the CMU Pronouncing Dictionary (`cmudict`), which we\n",
    "will use later in the MariaTTS project to turn words into sequences\n",
    "of **phones** (basic sound units).\n",
    "\n",
    "> This section is a gentle preview; we will come back to it in more detail when we work directly on speech.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import cmudict\n",
    "\n",
    "cmu = cmudict.dict()\n",
    "\n",
    "for word in [\"text\", \"processing\", \"python\", \"language\"]:\n",
    "    print(word, \"→\", cmu.get(word.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project connection\n",
    "\n",
    "- Each list of capitalized codes (like `['P', 'AY1', 'TH', 'AA0', 'N']`) represents one possible pronunciation of a word.\n",
    "- Later, the TTS system will break these sequences into **diphones** (pairs of phones) and use recorded audio for each diphone to synthesize speech.\n",
    "- Try adding some of your own words to the list above. Which ones are missing from the dictionary?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Part-of-speech (POS) tagging (preview)\n",
    "\n",
    "Part-of-speech tagging labels each word with its grammatical role (noun, verb, adjective, etc.). We will only take a quick look here.\n",
    "\n",
    "> Do not worry about understanding every tag now. The goal is just to get familiar with the idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tags = nltk.pos_tag(words)\n",
    "pos_tags[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6 (discussion)\n",
    "\n",
    "- Pick a few words and look at their tags.\n",
    "- Do the tags match your intuition about whether the word is a noun, verb, etc.?\n",
    "- Note down any tags you find confusing and any questions you have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary\n",
    "\n",
    "In this lesson, you:\n",
    "- Installed and imported NLTK\n",
    "- Loaded and printed a short text\n",
    "- Tokenized text into sentences and words\n",
    "- Normalized words with lowercasing\n",
    "- Counted word frequencies\n",
    "- Practised removing punctuation from token lists\n",
    "- Explored stemming and lemmatization\n",
    "- Looked up pronunciations using the CMU dictionary (preview for text-to-speech)\n",
    "- Took a quick look at part-of-speech tags\n",
    "\n",
    "These basic operations will be useful building blocks for later work with text in Python.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
